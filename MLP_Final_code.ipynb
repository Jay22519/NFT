{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "from random import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(input_shape,prev_shape,it) :\n",
    "    \n",
    "    weight = [[random() for i in range(prev_shape)] for j in range(input_shape)]\n",
    "    neuron = [random() for i in range(input_shape)]\n",
    "    bias = random()\n",
    "    \n",
    "    return weight,neuron,bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_sum(input,weight,bias) :\n",
    "    \n",
    "    activation = bias \n",
    "    for i in range(len(weight)) :\n",
    "        activation += (weight[i]*input[i]) \n",
    "        \n",
    "    return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(input) :\n",
    "    return 1.0/(1.0 + math.exp(-input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(network,input,bias,it) :\n",
    "    #print(it)\n",
    "    my_input = input\n",
    "    my_bias = bias \n",
    "    for i in range(it) :  #Each hidden layer\n",
    "        for j in range(len(network[i][1])): #For each neuron \n",
    "            sum_active = active_sum(network[i][0][j],my_input,my_bias)\n",
    "            activation = sigmoid(sum_active)\n",
    "                       \n",
    "            network[i][1][j] = activation\n",
    "            \n",
    "        my_input = network[i][1]\n",
    "        my_bias = network[i][2]\n",
    "                       \n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back_propagation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig_derivative(input) :\n",
    "    \n",
    "    return 1.0/(1.0 + math.exp(-input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(network,output,learning_rate,it,bias) :\n",
    "    #print(\"it is \",it,\"\\n\\n\")\n",
    "    \n",
    "    error = [[1 for i in range(100)] for j in range(it) ]\n",
    "    #First solving for last layer \n",
    "    it-=1 \n",
    "    sum_det = 0 \n",
    "    for i in range(len(network[it][1])): #Each neuron of last/output layer\n",
    "        \n",
    "        error[it][i] = output[i] -  network[it][1][i]\n",
    "        delta = error[it][i]*sig_derivative(network[it][1][i]) \n",
    "        error[it][i] = delta\n",
    "        sum_det += delta\n",
    "        #Updating all weights through which come to this neuron \n",
    "        \n",
    "        for j in range(len(network[it][0][i])) :\n",
    "            network[it][0][i][j] += (learning_rate*delta*network[it-1][1][j])\n",
    "\n",
    "            \n",
    "\n",
    "    #print(type(network[it]),network[it])\n",
    "    a = list(network[it-1]) \n",
    "    \n",
    "    a[2] = a[2]*(1 + (learning_rate*sum_det))\n",
    "    \n",
    "    network[it-1] = tuple(a)\n",
    "        \n",
    "    it-=1 \n",
    "    #Now doing the same thing for all networks until my it is 0 ,i.,e my input is original_input \n",
    "    while(it!=0) :\n",
    "        sum_det = 0 \n",
    "        for i in range(len(network[it][1])): #Each neuron of last/output layer\n",
    "\n",
    "            #Calculating error which is summation(error[it+1][j]*w[j][i])\n",
    "            err = 0 \n",
    "            for j in range(len(network[it+1][1])) : #All neurons of layers ahead \n",
    "                \n",
    "                err += (network[it+1][0][j][i]*error[it+1][j])\n",
    "            \n",
    "\n",
    "            error[it][i] = err \n",
    "            delta = error[it][1]*sig_derivative(network[it][1][i]) \n",
    "            error[it][i] = delta\n",
    "            sum_det += delta\n",
    "            #Updating all weights through which come to this neuron \n",
    "\n",
    "            for j in range(len(network[it][0][i])) :\n",
    "                network[it][0][i][j] += (learning_rate*delta*network[it-1][1][j])\n",
    "\n",
    "\n",
    "\n",
    "        #print(type(network[it]),network[it])\n",
    "        a = list(network[it-1]) \n",
    "\n",
    "        a[2] = a[2]*(1+(learning_rate*sum_det))\n",
    "\n",
    "        network[it-1] = tuple(a)\n",
    "\n",
    "        it-=1 \n",
    "        \n",
    "        \n",
    "    #Now doing for last layer\n",
    "    \n",
    "    \n",
    "    sum_det = 0 \n",
    "    for i in range(len(network[it][1])): #Each neuron of last/output layer\n",
    "\n",
    "        #Calculating error which is summation(error[it+1][j]*w[j][i])\n",
    "        err = 0 \n",
    "        for j in range(len(network[it+1][1])) : #All neurons of layers ahead \n",
    "\n",
    "            err += (network[it+1][0][j][i]*error[it+1][j])\n",
    "\n",
    "\n",
    "        error[it][i] = err \n",
    "        delta = error[it][1]*sig_derivative(network[it][1][i]) \n",
    "        error[it][i] = delta\n",
    "        sum_det += delta\n",
    "        #Updating all weights through which come to this neuron \n",
    "\n",
    "        for j in range(len(network[it][0][i])) :\n",
    "            network[it][0][i][j] += (learning_rate*delta*input[j])\n",
    "\n",
    "    \n",
    "    bias = bias*(1 + (learning_rate*sum_det))\n",
    "    \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    return network\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network,input,output,bias,learning_rate,it,epoch) :\n",
    "    leng = len(network)\n",
    "    for i in range(epoch) :\n",
    "        network = forward_prop(network,input,bias,it) \n",
    "        network = back_prop(network,output,learning_rate,it,bias) \n",
    "    #    print(network[leng-1][1])\n",
    "\n",
    "    \n",
    "    return network[leng - 1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6557472033075428, 0.2343183436854157]\n"
     ]
    }
   ],
   "source": [
    "input = [1,2,3,4,5] \n",
    "network = list()\n",
    "it = 0 \n",
    "network.append(add(3,5,it))\n",
    "it+=1\n",
    "network.append(add(2,3,it))\n",
    "it+=1\n",
    "output = [5,10]\n",
    "\n",
    "print(network[it-1][1])\n",
    "##Doing normalisation \n",
    "divide = 0 \n",
    "for i in range(len(output)) :\n",
    "    divide = output[i]**2 \n",
    "    \n",
    "divide = divide**0.5 \n",
    "for i in range(len(output)) :\n",
    "    output[i] = output[i]/divide\n",
    "    \n",
    "#Done with normalisation \n",
    "\n",
    "\n",
    "bias = 2 \n",
    "learning_rate = 0.05 \n",
    "\n",
    "\n",
    "answer = train(network,input,output,bias,learning_rate,it,1000)\n",
    "for i in range(len(answer)) :\n",
    "    answer[i]*=divide\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.015586682446566, 9.921294053381436]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
